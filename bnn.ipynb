{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: enron and chase manhattan bank sign long - term energy management\\nagreement\\nhouston and new york \\x01 ) enron energy services , a subsidiary of enron corp . ,\\nand the chase manhattan corporation ( nyse : cmb ) announced today a ten - year ,\\n$ 750 million energy management agreement . enron will provide energy\\nmanagement services for 860 of chase \\x01 , s facilities nationwide , including\\noffice buildings , retail branches and operations centers .\\nas part of this agreement , enron will provide commodity management , including\\nanalysis and consolidation of approximately 30 , 000 annual utility invoices\\ncurrently processed through multiple operating groups , as well as overall\\nproject management for energy infrastructure upgrades , designed to make\\nmaterial improvements in the energy efficiency of many chase facilities .\\n\" chase has taken a leadership position among financial institutions by\\nrecognizing the financial and administrative benefits of working with enron\\nto consolidate its energy management processes , \\x01 8 said lou pai , ceo of enron\\nenergy services . \\x01 & our agreement effectively takes chase out of the energy\\nmanagement business and enables them to focus more time and resources on\\ngrowing their successful financial services business . \"\\n\\x01 & this transaction is another great example of chase \\x01 , s discipline and\\ninitiative in creating significant value for our shareholders , \\x01 8 said liz\\nflynn , head of chase business services . \\x01 & it again demonstrates the\\neffectiveness of chase business services , a shared services organization\\ncreated to improve service and lower costs for all of chase \\x01 , s operating\\ngroups . \\x01 8\\nthe chase manhattan corporation , with $ 406 billion in assets , is one of the\\nworld \\x01 , s premier financial services institutions , with operations in 48\\ncountries around the globe . chase has a top - tier ranking in all areas of\\ninvestment banking , private banking , trading and global markets activities as\\nwell as information and transaction processing . chase is a leading provider\\nof financial solutions to large corporations , financial institutions ,\\ngovernment entities , middle market firms , small businesses and individuals ,\\nand has relationships with more than 30 million consumers across the united\\nstates through products and services such as credit cards , mortgages , online\\nbanking , debit cards , deposit products and auto loans . through its newly\\nformed business unit , chase . com , chase is successfully creating new business\\nmodels for the internet economy . chase can be reached on the web at\\nwww . chase . com .\\nenron is one of the world \\x01 , s leading electricity , natural gas and\\ncommunications companies . the company , which owns approximately $ 34 billion\\nin energy and communications assets , produces electricity and natural gas ,\\ndevelops , constructs and operates energy facilities worldwide , delivers\\nphysical commodities and financial and risk management services to customers\\naround the world , and is developing an intelligent network platform to\\nfacilitate online business . enron \\x01 , s internet address is www . enron . com , and\\nthe stock is traded under the ticker symbol \\x01 & ene . \\x01 8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('enron1/ham/0310.2000-02-02.farmer.ham.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    listOfTokens = re.split(r'\\W+', bigString)  # 用重复任意次的非字符作为切分标志\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]  # 将单词转化为小写\n",
    "\n",
    "def createVocabList(docList):# docList：每一封邮件转化成的字符串列表，其中包含的是可能有重复的小写单词\n",
    "    vocabSet = set([])# vocabSet：不重复的词汇表\n",
    "    for document in docList:\n",
    "        vocabSet = vocabSet | set(document)  #集合运算，通过取并集来保证不重复\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "data\\ham\n",
      "data\\spam\n"
     ]
    }
   ],
   "source": [
    "## 将数据以80%划为训练集和测试集\n",
    "train_ratio = 0.8\n",
    "train_list, test_list = [],[]\n",
    "dataroot = r\"data\"\n",
    "words = []\n",
    "class_flag = -1\n",
    "for a, b, f in os.walk(dataroot):\n",
    "    print(a)\n",
    "    for i in range(0, int(len(f)*train_ratio)):\n",
    "        train_text = os.path.join(a, f[i]) +'\\t'+str(class_flag)+'\\n'\n",
    "        train_list.append(train_text)\n",
    "        words.append(textParse(open(os.path.join(a, f[i]), 'r',encoding='gbk',errors='ignore').read()))\n",
    "    for i in range(int(len(f)*train_ratio),len(f)):\n",
    "        test_text = os.path.join(a, f[i]) +'\\t'+str(class_flag)+'\\n'\n",
    "        test_list.append(test_text)\n",
    "        words.append(textParse(open(os.path.join(a, f[i]), 'r',encoding='gbk',errors='ignore').read()))\n",
    "    class_flag += 1 \n",
    "random.shuffle(train_list)\n",
    "random.shuffle(test_list)\n",
    "\n",
    "vocablist = createVocabList(words)\n",
    "with open('train.txt', 'w') as f:\n",
    "    for train_text in train_list:\n",
    "        f.write(str(train_text))\n",
    "with open('test.txt','w') as f:\n",
    "    for test_text in test_list:\n",
    "        f.write(str(test_text))    #0是ham， 1是spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4605\n"
     ]
    }
   ],
   "source": [
    "len(vocablist)\n",
    "vocablist = sorted(vocablist)\n",
    "i=0\n",
    "while vocablist[i].isdigit():\n",
    "    i=i+1\n",
    "print(i) #从第4605开始 不再是数字 舍弃数字部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45161"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist = vocablist[4605:]\n",
    "len(vocablist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa',\n",
       " 'aaas',\n",
       " 'aabda',\n",
       " 'aabvmmq',\n",
       " 'aac',\n",
       " 'aachecar',\n",
       " 'aaer',\n",
       " 'aafco',\n",
       " 'aaiabe',\n",
       " 'aaigrcrb',\n",
       " 'aaihmqv',\n",
       " 'aaldano',\n",
       " 'aalland',\n",
       " 'aambique',\n",
       " 'aamlrg',\n",
       " 'aaoeuro',\n",
       " 'aare',\n",
       " 'aarhus',\n",
       " 'aaron',\n",
       " 'aashqcsny',\n",
       " 'aavilable',\n",
       " 'aaxrzm',\n",
       " 'aba',\n",
       " 'ababa',\n",
       " 'abacha',\n",
       " 'aback',\n",
       " 'abackof',\n",
       " 'abacus',\n",
       " 'abacustech',\n",
       " 'abandon',\n",
       " 'abandone',\n",
       " 'abandoned',\n",
       " 'abarch',\n",
       " 'abasements',\n",
       " 'abash',\n",
       " 'abashed',\n",
       " 'abate',\n",
       " 'abater',\n",
       " 'abazis',\n",
       " 'abb',\n",
       " 'abbas',\n",
       " 'abbasi',\n",
       " 'abbe',\n",
       " 'abbey',\n",
       " 'abbot',\n",
       " 'abbott',\n",
       " 'abbpge',\n",
       " 'abbreviation',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abcdef',\n",
       " 'abcdzhongguo',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominoplasty',\n",
       " 'abduct',\n",
       " 'abductbathe',\n",
       " 'abduction',\n",
       " 'abdul',\n",
       " 'abdv',\n",
       " 'abdvyyhtxnv',\n",
       " 'abe',\n",
       " 'abeckley',\n",
       " 'abed',\n",
       " 'abel',\n",
       " 'abelian',\n",
       " 'abelmosk',\n",
       " 'abelson',\n",
       " 'aber',\n",
       " 'abercrombie',\n",
       " 'aberdeen',\n",
       " 'abernathy',\n",
       " 'aberrant',\n",
       " 'aberrate',\n",
       " 'abet',\n",
       " 'abetted',\n",
       " 'abetting',\n",
       " 'abeyant',\n",
       " 'abfan',\n",
       " 'abhorred',\n",
       " 'abide',\n",
       " 'abideth',\n",
       " 'abidjan',\n",
       " 'abie',\n",
       " 'abigail',\n",
       " 'abiiity',\n",
       " 'abilene',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abissno',\n",
       " 'abject',\n",
       " 'abl',\n",
       " 'ablate',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ablished',\n",
       " 'ablution',\n",
       " 'abn',\n",
       " 'abner',\n",
       " 'abnormal',\n",
       " 'abnormalities',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'aboardca',\n",
       " 'abode',\n",
       " 'abolished',\n",
       " 'abolition',\n",
       " 'abominable',\n",
       " 'abominate',\n",
       " 'abonnement',\n",
       " 'abonnements',\n",
       " 'aborigine',\n",
       " 'aborning',\n",
       " 'abort',\n",
       " 'abortionists',\n",
       " 'abound',\n",
       " 'about',\n",
       " 'aboutnbsp',\n",
       " 'aboutus',\n",
       " 'aboutyour',\n",
       " 'above',\n",
       " 'aboveboard',\n",
       " 'aboveground',\n",
       " 'abovementioned',\n",
       " 'abpzhnpd',\n",
       " 'abr',\n",
       " 'abrade',\n",
       " 'abraham',\n",
       " 'abrahm',\n",
       " 'abram',\n",
       " 'abramo',\n",
       " 'abramson',\n",
       " 'abrasive',\n",
       " 'abraun',\n",
       " 'abrbr',\n",
       " 'abrbrbrbrp',\n",
       " 'abrbrfont',\n",
       " 'abreact',\n",
       " 'abreast',\n",
       " 'abridgment',\n",
       " 'abroad',\n",
       " 'abrogate',\n",
       " 'abrogates',\n",
       " 'abrupt',\n",
       " 'abruptness',\n",
       " 'abryou',\n",
       " 'abscess',\n",
       " 'abscissa',\n",
       " 'abscissae',\n",
       " 'absconded',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absense',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absenteeism',\n",
       " 'absently',\n",
       " 'absentminded',\n",
       " 'abshm',\n",
       " 'absoiuteiy',\n",
       " 'absolue',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutionelision',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbent',\n",
       " 'absorption',\n",
       " 'absorptionbark',\n",
       " 'abstain',\n",
       " 'abstention',\n",
       " 'abstinent',\n",
       " 'abstract',\n",
       " 'abstracter',\n",
       " 'abstractor',\n",
       " 'abstracts',\n",
       " 'abtyw',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'aburrell',\n",
       " 'abusable',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusive',\n",
       " 'abut',\n",
       " 'abutted',\n",
       " 'abutting',\n",
       " 'abvcm',\n",
       " 'abvps',\n",
       " 'abw',\n",
       " 'abyss',\n",
       " 'abyssinia',\n",
       " 'abzt',\n",
       " 'abzubestellen',\n",
       " 'abzuheben',\n",
       " 'aca',\n",
       " 'acacia',\n",
       " 'academe',\n",
       " 'academic',\n",
       " 'academician',\n",
       " 'academies',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acai',\n",
       " 'acal',\n",
       " 'acanthus',\n",
       " 'acapulco',\n",
       " 'acaso',\n",
       " 'acc',\n",
       " 'accede',\n",
       " 'accedepuerto',\n",
       " 'accedez',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'acceleration',\n",
       " 'accelerometer',\n",
       " 'accent',\n",
       " 'accentual',\n",
       " 'accentuate',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'acceptant',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'acceptor',\n",
       " 'acceptors',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessed',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accession',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accessrx',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accipiter',\n",
       " 'acclaim',\n",
       " 'acclamation',\n",
       " 'acclimate',\n",
       " 'acco',\n",
       " 'accolade',\n",
       " 'accomidations',\n",
       " 'accommodate',\n",
       " 'accommodates',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accomodatable',\n",
       " 'accomodate',\n",
       " 'accomodates',\n",
       " 'accomodations',\n",
       " 'accompaniment',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accompiish',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishers',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accompt',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accordant',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accost',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accountonline',\n",
       " 'accounts',\n",
       " 'accra',\n",
       " 'accredit',\n",
       " 'accreditate',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accretion',\n",
       " 'accretive',\n",
       " 'accross',\n",
       " 'accrual',\n",
       " 'accruals',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'accrues',\n",
       " 'accruing',\n",
       " 'accs',\n",
       " 'acct',\n",
       " 'acctg',\n",
       " 'accucast',\n",
       " 'acculturate',\n",
       " 'accum',\n",
       " 'accumuiate',\n",
       " 'accumuiated',\n",
       " 'accumuiation',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accumulativ',\n",
       " 'accumulative',\n",
       " 'accunom',\n",
       " 'accupuncture',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusatory',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accustom',\n",
       " 'accustomed',\n",
       " 'ace',\n",
       " 'acer',\n",
       " 'acerbic',\n",
       " 'acervantes',\n",
       " 'acetate',\n",
       " 'acetone',\n",
       " 'acetosoluble',\n",
       " 'acetylene',\n",
       " 'acf',\n",
       " 'acfrequency',\n",
       " 'ach',\n",
       " 'ache',\n",
       " 'acheive',\n",
       " 'acheivements',\n",
       " 'acheulian',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'aching',\n",
       " 'achokshi',\n",
       " 'achromatic',\n",
       " 'achter',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acidulous',\n",
       " 'aciphex',\n",
       " 'acjab',\n",
       " 'ack',\n",
       " 'ackaging',\n",
       " 'ackerman',\n",
       " 'ackley',\n",
       " 'acknowledge',\n",
       " 'acknowledgeable',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledging',\n",
       " 'acks',\n",
       " 'ackuyjpvd',\n",
       " 'acky',\n",
       " 'acm',\n",
       " 'acme',\n",
       " 'acoble',\n",
       " 'acoc',\n",
       " 'acock',\n",
       " 'acolyte',\n",
       " 'acooper',\n",
       " 'acordo',\n",
       " 'acorn',\n",
       " 'acorss',\n",
       " 'acosta',\n",
       " 'acostello',\n",
       " 'acouchette',\n",
       " 'acourtesy',\n",
       " 'acoustic',\n",
       " 'acp',\n",
       " 'acpw',\n",
       " 'acqainted',\n",
       " 'acquaint',\n",
       " 'acquaintance',\n",
       " 'acquainter',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'acquisitive',\n",
       " 'acquit',\n",
       " 'acquitted',\n",
       " 'acquring',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acredirect',\n",
       " 'acres',\n",
       " 'acrid',\n",
       " 'acrimonious',\n",
       " 'acrimony',\n",
       " 'acrobaat',\n",
       " 'acrobacy',\n",
       " 'acrobat',\n",
       " 'acronym',\n",
       " 'acronymbronchitis',\n",
       " 'acropolis',\n",
       " 'across',\n",
       " 'acrylate',\n",
       " 'acrylic',\n",
       " 'acs',\n",
       " 'acsdallas',\n",
       " 'acsqqylxwu',\n",
       " 'acsysinc',\n",
       " 'acsyxi',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'actinic',\n",
       " 'actinide',\n",
       " 'actinolite',\n",
       " 'actinometer',\n",
       " 'actinopod',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actipatch',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activation',\n",
       " 'activators',\n",
       " 'active',\n",
       " 'activeiy',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actobat',\n",
       " 'acton',\n",
       " 'actors',\n",
       " 'actos',\n",
       " 'actreses',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actua',\n",
       " 'actual',\n",
       " 'actualisation',\n",
       " 'actualization',\n",
       " 'actualize',\n",
       " 'actualized',\n",
       " 'actualizing',\n",
       " 'actuall',\n",
       " 'actually',\n",
       " 'actuals',\n",
       " 'actuarial',\n",
       " 'actuate',\n",
       " 'actuators',\n",
       " 'actuellement',\n",
       " 'acuff',\n",
       " 'acumen',\n",
       " 'acutally',\n",
       " 'acutals',\n",
       " 'acute',\n",
       " 'acutrim',\n",
       " 'acvbv',\n",
       " 'acy',\n",
       " 'acyc',\n",
       " 'acyclic',\n",
       " 'acyclovir',\n",
       " 'acylogen',\n",
       " 'acztffbr',\n",
       " 'ada',\n",
       " 'adage',\n",
       " 'adagio',\n",
       " 'adair',\n",
       " 'adakcz',\n",
       " 'adaklux',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adamik',\n",
       " 'adams',\n",
       " 'adamsck',\n",
       " 'adamson',\n",
       " 'adamvu',\n",
       " 'adan',\n",
       " 'adapt',\n",
       " 'adaptabie',\n",
       " 'adaptable',\n",
       " 'adaptation',\n",
       " 'adapter',\n",
       " 'adapters',\n",
       " 'adaptive',\n",
       " 'adbahadur',\n",
       " 'adbfaksaqw',\n",
       " 'add',\n",
       " 'addddds',\n",
       " 'added',\n",
       " 'addend',\n",
       " 'addenda',\n",
       " 'addict',\n",
       " 'addicts',\n",
       " 'addidas',\n",
       " 'adding',\n",
       " 'addio',\n",
       " 'addis',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'additiona',\n",
       " 'additional',\n",
       " 'additionall',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'additive',\n",
       " 'additon',\n",
       " 'addlrko',\n",
       " 'addr',\n",
       " 'address',\n",
       " 'addressbr',\n",
       " 'addressed',\n",
       " 'addressee',\n",
       " 'addresses',\n",
       " 'addressesbr',\n",
       " 'addressing',\n",
       " 'addressnbsp',\n",
       " 'adds',\n",
       " 'addtional',\n",
       " 'adduce',\n",
       " 'addul',\n",
       " 'adehlcgjik',\n",
       " 'adel',\n",
       " 'adela',\n",
       " 'adelaide',\n",
       " 'adele',\n",
       " 'adelia',\n",
       " 'adelphia',\n",
       " 'aden',\n",
       " 'adenine',\n",
       " 'adenoma',\n",
       " 'adenosine',\n",
       " 'adept',\n",
       " 'adequacy',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'aderso',\n",
       " 'adfa',\n",
       " 'adflower',\n",
       " 'adfttphh',\n",
       " 'adgbcfklxtnqyijzgvehm',\n",
       " 'adhere',\n",
       " 'adhering',\n",
       " 'adhesion',\n",
       " 'adhesive',\n",
       " 'adi',\n",
       " 'adiabatic',\n",
       " 'adiatec',\n",
       " 'adieu',\n",
       " 'adieusmell',\n",
       " 'adiitional',\n",
       " 'adios',\n",
       " 'adip',\n",
       " 'adipe',\n",
       " 'adipex',\n",
       " 'adipic',\n",
       " 'adipren',\n",
       " 'adiprenl',\n",
       " 'adirondack',\n",
       " 'adivasi',\n",
       " 'adjacent',\n",
       " 'adjectival',\n",
       " 'adjoin',\n",
       " 'adjourn',\n",
       " 'adjudge',\n",
       " 'adjudgepluperfect',\n",
       " 'adjudications',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjusted',\n",
       " 'adjustements',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adjutant',\n",
       " 'adkaw',\n",
       " 'adkins',\n",
       " 'adler',\n",
       " 'adlpex',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'administer',\n",
       " 'administered',\n",
       " 'administering',\n",
       " 'administrate',\n",
       " 'administration',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'administrators',\n",
       " 'administratrix',\n",
       " 'adminsystem',\n",
       " 'admirably',\n",
       " 'admiral',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admission',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'admitters',\n",
       " 'admitting',\n",
       " 'admix',\n",
       " 'admixture',\n",
       " 'admonish',\n",
       " 'admonishing',\n",
       " 'admonition',\n",
       " 'ado',\n",
       " 'adoactress',\n",
       " 'adobbe',\n",
       " 'adobe',\n",
       " 'adoiescents',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolph',\n",
       " 'adolphus',\n",
       " 'adonis',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adoption',\n",
       " 'adoptions',\n",
       " 'adoptive',\n",
       " 'adopts',\n",
       " 'adore',\n",
       " 'adornments',\n",
       " 'adposition',\n",
       " 'adr',\n",
       " 'adress',\n",
       " 'adrial',\n",
       " 'adrian',\n",
       " 'adriana',\n",
       " 'adriatic',\n",
       " 'adrienne',\n",
       " 'adrienneolsen',\n",
       " 'adrift',\n",
       " 'adroit',\n",
       " 'adroitpratt',\n",
       " 'adrrf',\n",
       " 'ads',\n",
       " 'adsero',\n",
       " 'adsl',\n",
       " 'adslsapo',\n",
       " 'adso',\n",
       " 'adsorption',\n",
       " 'adsorptive',\n",
       " 'adsyour',\n",
       " 'adt',\n",
       " 'adtfa',\n",
       " 'adtra',\n",
       " 'adufod',\n",
       " 'aduit',\n",
       " 'adulate',\n",
       " 'adult',\n",
       " 'adulterate',\n",
       " 'adultery',\n",
       " 'adultfreely',\n",
       " 'adults',\n",
       " 'adv',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancements',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advanstar',\n",
       " 'advantage',\n",
       " 'advantageous',\n",
       " 'advantages',\n",
       " 'advantaqes',\n",
       " 'adventitious',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventuresome',\n",
       " 'adventurous',\n",
       " 'adver',\n",
       " 'adverb',\n",
       " 'adverbs',\n",
       " 'advergaming',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'advert',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisements',\n",
       " 'advertiser',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advil',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advisee',\n",
       " 'advisees',\n",
       " 'advisement',\n",
       " 'advisements',\n",
       " 'adviser',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advisory',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'advolt',\n",
       " 'adware',\n",
       " 'adwords',\n",
       " 'adz',\n",
       " 'aeavgwvhr',\n",
       " 'aebdb',\n",
       " 'aec',\n",
       " 'aeciospore',\n",
       " 'aecnuvxnv',\n",
       " 'aed',\n",
       " 'aegean',\n",
       " 'aegis',\n",
       " 'aeh',\n",
       " 'aekdju',\n",
       " 'aeli',\n",
       " 'aenax',\n",
       " 'aeneas',\n",
       " 'aeneid',\n",
       " 'aenetsell',\n",
       " 'aeolian',\n",
       " 'aeor',\n",
       " 'aep',\n",
       " 'aepin',\n",
       " 'aerate',\n",
       " 'aerial',\n",
       " 'aerobacter',\n",
       " 'aerobic',\n",
       " 'aerodrome',\n",
       " 'aerodynamic',\n",
       " 'aerofoam',\n",
       " 'aerofoammetals',\n",
       " 'aerogene',\n",
       " 'aeromarine',\n",
       " 'aeronautic',\n",
       " 'aerospace',\n",
       " 'aerrwzu',\n",
       " 'aeruginosa',\n",
       " 'aeschylus',\n",
       " 'aesop',\n",
       " 'aesthete',\n",
       " 'aesthetic',\n",
       " 'aetjs',\n",
       " 'aez',\n",
       " 'aezzxa',\n",
       " 'afar',\n",
       " 'afcozrxxc',\n",
       " 'afdeling',\n",
       " 'afeslb',\n",
       " 'aff',\n",
       " 'affable',\n",
       " 'affair',\n",
       " 'affaires',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affectate',\n",
       " 'affectation',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affectingly',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affections',\n",
       " 'affective',\n",
       " 'affects',\n",
       " 'affelt',\n",
       " 'afferent',\n",
       " 'affiance',\n",
       " 'affidavit',\n",
       " 'affidavits',\n",
       " 'affiiiate',\n",
       " 'affiiiated',\n",
       " 'affiliate',\n",
       " 'affiliated',\n",
       " 'affiliates',\n",
       " 'affine',\n",
       " 'affinity',\n",
       " 'affirm',\n",
       " 'affirmation',\n",
       " 'affirmative',\n",
       " 'affirmatively',\n",
       " 'affirmed',\n",
       " 'affirms',\n",
       " 'affix',\n",
       " 'affixation',\n",
       " 'afflict',\n",
       " 'affliction',\n",
       " 'afflictions',\n",
       " 'affluence',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'affordableluxury',\n",
       " 'afforded',\n",
       " 'affords',\n",
       " 'afforest',\n",
       " 'afforestation',\n",
       " 'affricate',\n",
       " 'affront',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afgkietc',\n",
       " 'afh',\n",
       " 'aficionado',\n",
       " 'afield',\n",
       " 'afio',\n",
       " 'afk',\n",
       " 'aflame',\n",
       " 'afloat',\n",
       " 'aflods',\n",
       " 'afmzs',\n",
       " 'afo',\n",
       " 'afont',\n",
       " 'afoot',\n",
       " 'aforementioned',\n",
       " 'aforesaid',\n",
       " 'aforesaidcruz',\n",
       " 'aforethought',\n",
       " 'afoul',\n",
       " 'afraction',\n",
       " 'afraid',\n",
       " 'afreedomites',\n",
       " 'afresh',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'afro',\n",
       " 'afrol',\n",
       " 'afruhling',\n",
       " 'afsjlzbj',\n",
       " 'aft',\n",
       " 'after',\n",
       " 'afterbirth',\n",
       " 'aftereffect',\n",
       " 'afterimage',\n",
       " 'afteriwokeaabb',\n",
       " 'afterlife',\n",
       " 'aftermarket',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afternooncirculate',\n",
       " 'afternoonfatty',\n",
       " 'afternoons',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'afwkny',\n",
       " 'agaain',\n",
       " 'agaal',\n",
       " 'agah',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agalloch',\n",
       " 'agaln',\n",
       " 'agamemnon',\n",
       " 'agate',\n",
       " 'agatha',\n",
       " 'agave',\n",
       " 'agbohyq',\n",
       " 'agdxxw',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agee',\n",
       " 'ageless',\n",
       " 'agemqzt',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agfa',\n",
       " 'aggie',\n",
       " 'aggiebob',\n",
       " 'aggiel',\n",
       " 'aggiemom',\n",
       " 'aggierob',\n",
       " 'aggies',\n",
       " 'aggietx',\n",
       " 'agglomerate',\n",
       " 'agglutinate',\n",
       " 'aggragate',\n",
       " 'aggravate',\n",
       " 'aggravateboylston',\n",
       " 'aggregate',\n",
       " 'aggregating',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aggressor',\n",
       " 'aggz',\n",
       " 'aghqspv',\n",
       " 'agii',\n",
       " 'agile',\n",
       " 'aging',\n",
       " 'agitate',\n",
       " 'agitating',\n",
       " 'agitation',\n",
       " 'agjgel',\n",
       " 'agjwxor',\n",
       " 'agleam',\n",
       " 'agm',\n",
       " 'agmt',\n",
       " 'agmts',\n",
       " 'agnd',\n",
       " 'agnes',\n",
       " 'agnew',\n",
       " 'agnomen',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'agouti',\n",
       " 'agoyf',\n",
       " 'agr',\n",
       " 'agra',\n",
       " 'agraffe',\n",
       " 'agrandissement',\n",
       " 'agrapha',\n",
       " 'agrarian',\n",
       " 'agrbrsnq',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeement',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agreementat',\n",
       " 'agreements',\n",
       " 'agreemtent',\n",
       " 'agrees',\n",
       " 'agrestal',\n",
       " 'agricola',\n",
       " 'agricultural',\n",
       " 'agriculture',\n",
       " 'agrimony',\n",
       " 'agrjhxb',\n",
       " 'agrment',\n",
       " 'agrra',\n",
       " 'agrx',\n",
       " 'ags',\n",
       " 'agtaylor',\n",
       " 'agua',\n",
       " 'aguayo',\n",
       " 'aguilera',\n",
       " 'aguirre',\n",
       " 'agumadu',\n",
       " 'aha',\n",
       " 'ahckf',\n",
       " 'ahe',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'aheqp',\n",
       " 'ahernand',\n",
       " 'aherrera',\n",
       " 'ahg',\n",
       " 'ahhaprol',\n",
       " 'ahihqe',\n",
       " 'ahk',\n",
       " 'ahkap',\n",
       " 'ahlhckrb',\n",
       " 'ahm',\n",
       " 'ahmadabad',\n",
       " 'ahmed',\n",
       " 'ahmedabad',\n",
       " 'ahn',\n",
       " 'aho',\n",
       " 'ahochuzmlzgkxmzcp',\n",
       " 'ahoy',\n",
       " 'ahwsil',\n",
       " 'aiberta',\n",
       " 'aicy',\n",
       " 'aid',\n",
       " 'aida',\n",
       " 'aide',\n",
       " 'aides',\n",
       " 'aids',\n",
       " 'aiert',\n",
       " 'aifbs',\n",
       " 'aifhhs',\n",
       " 'aig',\n",
       " 'aigqykkq',\n",
       " 'aiity',\n",
       " 'aij',\n",
       " 'aiken',\n",
       " 'ail',\n",
       " 'ailairmen',\n",
       " 'ailanthus',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocablist.txt', 'w') as f:\n",
    "    for word in vocablist:\n",
    "        f.write(word+'\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45160"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(open('vocablist.txt', 'r').read().strip().split('\\t'))[1:]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据个数： 4137\n",
      "测试集数据个数： 1035\n"
     ]
    }
   ],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, txt_path, vocab):\n",
    "        self.text_info = self.get_info(txt_path)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def get_info(self, txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "                text_info = f.readlines()\n",
    "                text_info = list(map(lambda x:x.strip().split('\\t'), text_info))\n",
    "        return text_info\n",
    "\n",
    "\n",
    "    # 将字符串转换为列表，0表示字符在词汇表中不存在，1表示字符在词汇表中存在\n",
    "    def Words2Vec(self, inputSet):# vocabList：词汇表；inputSet：输入的字符串列表\n",
    "        returnVec = [0] * 45160\n",
    "        listOfTokens = re.split(r'\\W+', inputSet)  # 用重复任意次的非字符作为切分标志\n",
    "        inputSet = [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "        for word in inputSet:\n",
    "            if word in self.vocab:\n",
    "                returnVec[self.vocab.index(word)] += 1\n",
    "        return torch.Tensor(returnVec).reshape(1,1,45160)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        email_path, label = self.text_info[index]\n",
    "        email = open(email_path, 'r',encoding='gbk',errors='ignore').read()\n",
    "        email = self.Words2Vec(email)\n",
    "        label = int(label)\n",
    "        return email, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_info)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data = LoadData(\"train.txt\", vocab)\n",
    "    print(\"训练集数据个数：\",len(train_data))\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=128, shuffle=True, drop_last = True)\n",
    "    test_data = LoadData(\"test.txt\", vocab)\n",
    "    print(\"测试集数据个数：\",len(test_data))\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_BBB(nn.Module):\n",
    "    \"\"\"\n",
    "        Layer of our BNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, output_features, prior_var=1.):\n",
    "        \"\"\"\n",
    "            Initialization of our layer : our prior is a normal distribution\n",
    "            centered in 0 and of variance 20.\n",
    "        \"\"\"\n",
    "        # initialize layers\n",
    "        super().__init__()\n",
    "        # set input and output dimensions\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # initialize mu and rho parameters for the weights of the layer\n",
    "        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "\n",
    "        #initialize mu and rho parameters for the layer's bias\n",
    "        self.b_mu =  nn.Parameter(torch.zeros(output_features))\n",
    "        self.b_rho = nn.Parameter(torch.zeros(output_features))        \n",
    "\n",
    "        #initialize weight samples (these will be calculated whenever the layer makes a prediction)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "        # initialize prior distribution for all of the weights and biases\n",
    "        self.prior = Normal(0,prior_var)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "          Optimization process\n",
    "        \"\"\"\n",
    "        # sample weights\n",
    "        w_epsilon = Normal(0,1).sample(self.w_mu.shape)\n",
    "        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon\n",
    "\n",
    "        # sample bias\n",
    "        b_epsilon = Normal(0,1).sample(self.b_mu.shape)\n",
    "        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon\n",
    "\n",
    "        # record log prior by evaluating log pdf of prior at sampled weight and bias\n",
    "        w_log_prior = self.prior.log_prob(self.w)\n",
    "        b_log_prior = self.prior.log_prob(self.b)\n",
    "        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)\n",
    "\n",
    "        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values\n",
    "        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)))\n",
    "        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)))\n",
    "        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()\n",
    "\n",
    "        return F.linear(input, self.w, self.b)\n",
    "\n",
    "class MLP_BBB(nn.Module):\n",
    "    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):\n",
    "\n",
    "        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer\n",
    "        super().__init__()\n",
    "        self.hidden = Linear_BBB(45160,hidden_units, prior_var=prior_var)\n",
    "        self.out = Linear_BBB(hidden_units, 45160, prior_var=prior_var)\n",
    "        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        # again, this is equivalent to a standard multilayer perceptron\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self):\n",
    "        # calculate the log prior over all the layers\n",
    "        return self.hidden.log_prior + self.out.log_prior\n",
    "\n",
    "    def log_post(self):\n",
    "        # calculate the log posterior over all the layers\n",
    "        return self.hidden.log_post + self.out.log_post\n",
    "\n",
    "    def sample_elbo(self, input, target, samples):\n",
    "        # we calculate the negative elbo, which will be our loss function\n",
    "        #initialize tensors\n",
    "        outputs = torch.zeros(samples, target.shape[0])\n",
    "        log_priors = torch.zeros(samples)\n",
    "        log_posts = torch.zeros(samples)\n",
    "        log_likes = torch.zeros(samples)\n",
    "        # make predictions and calculate prior, posterior, and likelihood for a given number of samples\n",
    "        for i in range(samples):\n",
    "            outputs[i] = self(input).reshape(-1) # make predictions\n",
    "            log_priors[i] = self.log_prior() # get log prior\n",
    "            log_posts[i] = self.log_post() # get log variational posterior\n",
    "            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood\n",
    "        # calculate monte carlo estimate of prior posterior and likelihood\n",
    "        log_prior = log_priors.mean()\n",
    "        log_post = log_posts.mean()\n",
    "        log_like = log_likes.mean()\n",
    "        # calculate the negative elbo (which is our loss function)\n",
    "        loss = log_post - log_prior - log_like\n",
    "        return loss\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# x = torch.unsqueeze(torch.linspace(-1,1,100),dim=1)\n",
    "# y = x.pow(3)+0.1*torch.randn(x.size())\n",
    "\n",
    "net = MLP_BBB(48, prior_var=10)\n",
    "optimizer = optim.Adam(net.parameters(), lr=.1)\n",
    "epochs = 5000\n",
    "hist_epochs = np.zeros((int(epochs/10),1))\n",
    "hist_loss = np.zeros((int(epochs/10),1))\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,45160), data[1])\n",
    "        loss = net.sample_elbo(x, y, 1)\n",
    "      loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        hist_loss[int(epoch/10)] = loss.data\n",
    "        hist_epochs[int(epoch/10)] = epoch+1\n",
    "        print('epoch: {}/{}'.format(epoch+1,epochs))\n",
    "        print('Loss: %.4f' % loss.item())\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is the number of \"predictions\" we make for 1 x-value.\n",
    "samples = 500\n",
    "x_tmp = torch.linspace(-2,2,100).reshape(-1,1)\n",
    "y_samp = np.zeros((samples,100))\n",
    "for s in range(samples):\n",
    "    y_tmp = net(x_tmp).detach().numpy()\n",
    "    y_samp[s] = y_tmp.reshape(-1)\n",
    "\n",
    "\n",
    "y_sigma = np.std(y_samp, axis=0)\n",
    "y_mean = np.mean(y_samp, axis = 0)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(x_tmp.numpy(), y_mean, label='Mean Posterior Predictive')\n",
    "plt.fill_between(x_tmp.numpy().reshape(-1), y_mean + 2 * y_sigma,  y_mean - 2 * y_sigma, alpha=0.25, label='Epistemic uncertainty')\n",
    "plt.legend()\n",
    "plt.scatter(x, y)\n",
    "plt.title('Posterior Predictive')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(hist_epochs,hist_loss, label='Loss Function')\n",
    "plt.legend()\n",
    "plt.title('The Value of loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(45160, 512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    \n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "    \n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight)\n",
    "    outw_sigma = torch.randn_like(net.out.weight)\n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias)\n",
    "    outb_sigma = torch.randn_like(net.out.bias)\n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 1129, 40])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for data ,label in train_loader:\n",
    "    print(data.shape)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# calculate the loss and take a gradient step\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m45160\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m normalizer_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     10\u001b[0m total_epoch_loss_train \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m normalizer_train\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\pyro\\infer\\svi.py:75\u001b[0m, in \u001b[0;36mSVI.step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[1;32m---> 75\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained()\n\u001b[0;32m     78\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\pyro\\infer\\trace_elbo.py:142\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainable_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(surrogate_elbo_particle, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    141\u001b[0m         surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39msurrogate_elbo_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n\u001b[1;32m--> 142\u001b[0m         \u001b[43msurrogate_loss_particle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39melbo\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_isnan(loss):\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 5\n",
    "loss = 0\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,45160), data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    \n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m predict(images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m))\n\u001b[0;32m     14\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "num_samples = 2\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.view(-1,1129*40))\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction when network can decide not to predict\n",
    "\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "\n",
    "print(\"Total images: \", total)\n",
    "print(\"Skipped: \", total-total_predicted_for)\n",
    "print(\"Accuracy when made predictions: %d %%\" % (100 * correct / total_predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "def give_uncertainities(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [F.log_softmax(model(x.view(-1,28*28)).data, 1).detach().numpy() for model in sampled_models]\n",
    "    return np.asarray(yhats)\n",
    "    #mean = torch.mean(torch.stack(yhats), 0)\n",
    "    #return np.argmax(mean, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
